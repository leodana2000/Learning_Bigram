{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "t.set_num_threads(8)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from train import train\n",
    "from models import Transformer, Low_rank, AoT\n",
    "from utils import generate_data, entropy, power_unif_law\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training Transformer.\"\"\"\n",
    "\n",
    "# Transformer parameters.\n",
    "N = 50\n",
    "d = 15\n",
    "nb_layers = 1\n",
    "width = 0\n",
    "depth = 1\n",
    "para = 20\n",
    "d_head = 10\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[N, N, 1]\n",
    "t.manual_seed(2222)\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Learning parameters for the Transformer.\n",
    "batch_size=2**10\n",
    "num_batch=1000\n",
    "epoches=10\n",
    "lr=1e-3\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "# Training the Transformer.\n",
    "model = Transformer(d, N, nb_layers, width, depth, para, d_head, nb_head, context_window, pi)\n",
    "Dict = train(model, Data, epoches, lr=lr, next_token=True)\n",
    "plt.plot(Dict['Loss'])\n",
    "\n",
    "# Upper bound: we compute the divergence with the uniform predictor.\n",
    "ent=entropy(pi)\n",
    "plt.plot([np.log(N)-ent for _ in Dict['Loss']], label='Uniform baseline', color='red')\n",
    "\n",
    "# Learning parameters for the sequence encoder.\n",
    "low_batch_size=2**10\n",
    "low_num_batch=1000\n",
    "low_lr=1e-3\n",
    "epochs=4\n",
    "\n",
    "# Lower bound: we compute the best Sequence encoder, the diverge of which sets the attainable lower bound.\n",
    "model_low = Low_rank(d, N, context_window, pi)\n",
    "Data = generate_data(low_batch_size, low_num_batch, pi, context_window)\n",
    "dict_low = train(model_low, Data, epochs, lr=low_lr)\n",
    "best_loss = sum(dict_low['Loss'][-101:-1])/100\n",
    "plt.plot([best_loss for _ in Dict['Loss']], label='Optimal baseline', color='green')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Divergence\")\n",
    "plt.title(\"Transformer's learning dynamics\")\n",
    "plt.show()\n",
    "\n",
    "# We plot the accuracy of the Transformer, the accuracy of the random predictor, \n",
    "# and the lower bound from are paper as well as the previous sota bound (in accuracy, not in worst-case).\n",
    "plt.plot(Dict['Acc'], label=f'Next token')\n",
    "plt.plot([1/N for _ in Dict['Acc']], color='black', label='Random baseline')\n",
    "plt.plot([1/N+(1-1/N)*para*d_head/(N**(n_gram-1)) for _ in Dict['Acc']], label='Our Lower bound')\n",
    "plt.plot([1/N+(1-1/N)*(para*(d_head-1)+1)/(N**(n_gram-1)) for _ in Dict['Acc']], label='Previous lower bound')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(top=1+0.1, bottom=0-0.1)\n",
    "plt.title(\"Transformer's learning dynamics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Scaling laws for the Memorization Capacity of an AoT.\"\"\"\n",
    "\n",
    "# Choice of the experiment: \n",
    "# - 1 for the case d=d_head, \n",
    "# - 2 for the case d fixed, \n",
    "# - 3 for the case d_head fixed,\n",
    "exp_number = 2\n",
    "if exp_number == 1:\n",
    "    parameter_list = [(50, d, d, 31, 1, 5) for d in range(3, 13+1)]\n",
    "elif exp_number == 2:\n",
    "    parameter_list = [(50, 10, d_head, 8, 5, 3) for d_head in [10, 11, 12, 13, 15, 16]]\n",
    "elif exp_number == 3:\n",
    "    parameter_list = [(50, d, 10, 21, 5, 4) for d in range(5, 15+1, 2)]\n",
    "elif exp_number == 4:\n",
    "    parameter_list = [(10, 10, 2*20*10, 1*2*10, 20)]\n",
    "\n",
    "# Model parameters.\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[100, 100, 1]\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**9\n",
    "num_batch=1000\n",
    "lr=1e-3\n",
    "epochs=20\n",
    "\n",
    "repetition = 2\n",
    "\n",
    "N = 50\n",
    "t.manual_seed(2222)\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "for i, (N, d, d_head, max_para, min_para, step) in enumerate(parameter_list):\n",
    "    \n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "    width_list = []\n",
    "\n",
    "    for para in tqdm(range(min_para, max_para+1, step)):\n",
    "        accuracy = 0\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            if exp_number == 4:\n",
    "                width = para\n",
    "                model = Transformer(d, N, nb_layers, width, 1, 1, d_head, nb_head, context_window, pi)\n",
    "            else:\n",
    "                model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        if exp_number == 4:\n",
    "            width_list.append(width)\n",
    "            para_list.append(1)\n",
    "        else:\n",
    "            para_list.append(para)\n",
    "            width_list.append(0)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "        'width': width_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Exp_{6+i}_{exp_number}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:40<00:00, 16.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23080078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:36<00:00, 15.65s/it]\n",
      "  9%|▉         | 1/11 [05:16<52:49, 316.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.440439453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:55<00:00, 17.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.280224609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:57<00:00, 17.76s/it]\n",
      " 18%|█▊        | 2/11 [11:10<50:43, 338.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56044921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:59<00:00, 18.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.367734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:59<00:00, 17.92s/it]\n",
      " 27%|██▋       | 3/11 [17:09<46:22, 347.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:04<00:00, 18.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46947265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:04<00:00, 18.42s/it]\n",
      " 36%|███▋      | 4/11 [23:18<41:33, 356.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9389453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:16<00:00, 19.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55154296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:16<00:00, 19.69s/it]\n",
      " 45%|████▌     | 5/11 [29:51<36:58, 369.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1030859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:21<00:00, 20.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.671240234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:22<00:00, 20.28s/it]\n",
      " 55%|█████▍    | 6/11 [36:35<31:46, 381.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34248046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:29<00:00, 20.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.769296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:29<00:00, 20.92s/it]\n",
      " 64%|██████▎   | 7/11 [43:34<26:13, 393.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.53859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:49<00:00, 22.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:33<00:00, 21.38s/it]\n",
      " 73%|███████▎  | 8/11 [50:56<20:27, 409.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.67296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:45<00:00, 22.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:47<00:00, 22.78s/it]\n",
      " 82%|████████▏ | 9/11 [58:30<14:06, 423.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.62578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:55<00:00, 23.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.846923828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:03<00:00, 24.38s/it]\n",
      " 91%|█████████ | 10/11 [1:06:30<07:20, 440.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.69384765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:49<00:00, 22.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88603515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:43<00:00, 22.33s/it]\n",
      "100%|██████████| 11/11 [1:14:03<00:00, 403.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7720703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Scaling laws for the Memorization Capacity of an AoT.\"\"\"\n",
    "\n",
    "# Choice of the experiment: \n",
    "# - 1 for the case d=d_head, \n",
    "# - 2 for the case d fixed, \n",
    "# - 3 for the case d_head fixed,\n",
    "exp_number = 2\n",
    "min_d_head = 5\n",
    "max_d_head = 15\n",
    "step = 1\n",
    "\n",
    "# Model parameters.\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[100, 100, 1]\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**10\n",
    "num_batch=1000\n",
    "lr=1e-3\n",
    "epochs=10\n",
    "\n",
    "repetition = 2\n",
    "\n",
    "N = 50\n",
    "d = 10\n",
    "para = 20\n",
    "t.manual_seed(2222)\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "d_head_list = []\n",
    "\n",
    "for d_head in tqdm(range(min_d_head, max_d_head+1, step)):\n",
    "    accuracy = 0\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "        accuracy += acc\n",
    "        print(accuracy)\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    d_head_list.append(d_head)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Exp_{-1}_{exp_number}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"Scaling laws for the Memorization Capacity of an AoT.\"\"\"\n",
    "\n",
    "# Choice of the experiment: \n",
    "# - 1 for the case d=d_head, \n",
    "# - 2 for the case d fixed, \n",
    "# - 3 for the case d_head fixed,\n",
    "exp_number = 3\n",
    "min_d = 5\n",
    "max_d = 25\n",
    "step = 1\n",
    "\n",
    "# Model parameters.\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[100, 100, 1]\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**10\n",
    "num_batch=1000\n",
    "lr=1e-3\n",
    "epochs=10\n",
    "\n",
    "repetition = 1\n",
    "\n",
    "N = 50\n",
    "d_head = 10\n",
    "para = 20\n",
    "t.manual_seed(2222)\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "d_head_list = []\n",
    "\n",
    "for d in tqdm(range(min_d, max_d+1, step)):\n",
    "    accuracy = 0\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "        accuracy += acc\n",
    "        print(accuracy)\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    d_head_list.append(d_head)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_3_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
