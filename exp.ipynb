{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from train import train\n",
    "from models import Transformer, Low_rank, AoT\n",
    "from utils import generate_data, entropy, power_unif_law\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leodana/anaconda3/envs/leo_env/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]/Users/leodana/anaconda3/envs/leo_env/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 5/20 [01:19<03:59, 15.94s/it]"
     ]
    }
   ],
   "source": [
    "\"\"\"Training Transformer.\"\"\"\n",
    "\n",
    "# Transformer parameters.\n",
    "N = 50\n",
    "d = 10\n",
    "nb_layers = 1\n",
    "width = 0\n",
    "depth = 1\n",
    "para = 22\n",
    "d_head = 8\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[N, N, 1]\n",
    "t.manual_seed(2222)\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Learning parameters for the Transformer.\n",
    "batch_size=2**9\n",
    "num_batch=1000\n",
    "epoches=20\n",
    "lr=1e-3\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "# Training the Transformer.\n",
    "model = Transformer(d, N, nb_layers, width, depth, para, d_head, nb_head, context_window, pi)\n",
    "Dict = train(model, Data, epoches, lr=lr, next_token=True)\n",
    "plt.plot(Dict['Loss'])\n",
    "\n",
    "# Upper bound: we compute the divergence with the uniform predictor.\n",
    "ent=entropy(pi)\n",
    "plt.plot([np.log(N)-ent for _ in Dict['Loss']], label='Uniform baseline', color='red')\n",
    "\n",
    "# Learning parameters for the sequence encoder.\n",
    "low_batch_size=2**10\n",
    "low_num_batch=1000\n",
    "low_lr=1e-3\n",
    "epochs=4\n",
    "\n",
    "# Lower bound: we compute the best Sequence encoder, the diverge of which sets the attainable lower bound.\n",
    "model_low = Low_rank(d, N, context_window, pi)\n",
    "Data = generate_data(low_batch_size, low_num_batch, pi, context_window)\n",
    "dict_low = train(model_low, Data, epochs, lr=low_lr)\n",
    "best_loss = sum(dict_low['Loss'][-101:-1])/100\n",
    "plt.plot([best_loss for _ in Dict['Loss']], label='Optimal baseline', color='green')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Divergence\")\n",
    "plt.title(\"Transformer's learning dynamics\")\n",
    "plt.show()\n",
    "\n",
    "# We plot the accuracy of the Transformer, the accuracy of the random predictor, \n",
    "# and the lower bound from are paper as well as the previous sota bound (in accuracy, not in worst-case).\n",
    "plt.plot(Dict['Acc'], label=f'Next token')\n",
    "plt.plot([1/N for _ in Dict['Acc']], color='black', label='Random baseline')\n",
    "plt.plot([1/N+(1-1/N)*para*d_head/(N**(n_gram-1)) for _ in Dict['Acc']], label='Our Lower bound')\n",
    "plt.plot([1/N+(1-1/N)*(para*(d_head-1)+1)/(N**(n_gram-1)) for _ in Dict['Acc']], label='Previous lower bound')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(top=1+0.1, bottom=0-0.1)\n",
    "plt.title(\"Transformer's learning dynamics\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
