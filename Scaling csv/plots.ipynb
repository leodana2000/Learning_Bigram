{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from typing import List\n",
    "from colour import Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the average d over N\n",
    "data_d_N: pd.DataFrame = pd.read_csv('scaling_d_N_new.csv')\n",
    "for N in [10, 20, 30, 50, 100]:\n",
    "    group = data_d_N[data_d_N['N'] == N].groupby(['d'])\n",
    "    loss_group = group.mean()['loss']\n",
    "    X = [d for d in loss_group.index.to_list()]\n",
    "    Z = loss_group.to_list()\n",
    "    #minZ = min(Z)\n",
    "    #Z = [z-minZ for z in Z]\n",
    "    plt.plot(X, Z, label=f'N={N}')\n",
    "    plt.xlabel('d')\n",
    "    plt.ylabel('KL divergence') #(rescaled)')\n",
    "plt.title('Scaling law on the residual dimension (d) and the number of tokens (N)')\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the average h over d\n",
    "data_d_h: pd.DataFrame = pd.read_csv('scaling_d_h_new.csv')\n",
    "for d in [2, 4, 6, 8, 10]:\n",
    "    group = data_d_h[data_d_h['d'] == d].groupby(['h'])\n",
    "    loss_group = group.mean()['loss']\n",
    "    X = [h for h in loss_group.index.to_list()]\n",
    "    Z = loss_group.to_list()\n",
    "    plt.plot(X, Z, label=f'd={d}')\n",
    "plt.xlabel('Ratio h/d')\n",
    "plt.ylabel('KL divergence')\n",
    "plt.title('Scaling law on the residual dimension (d) and the hidden dimension (h)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the average nb_layers over d\n",
    "data_d_layer: pd.DataFrame = pd.read_csv('scaling_d_layer_new.csv')\n",
    "list_d: List[int] = data_d_layer.groupby(['d']).mean()['loss'].index.to_list()\n",
    "for d in list_d:\n",
    "    group = data_d_layer[data_d_layer['d'] == d].groupby(['nb_layers'])\n",
    "    loss_group = group.mean()['loss']\n",
    "    X = loss_group.index.to_list()\n",
    "    Z = loss_group.to_list()\n",
    "    plt.plot(X, Z, label=f'd={d}')\n",
    "plt.xlabel('Number of layers')\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.ylabel('KL divergence')\n",
    "plt.title('Scaling law on the residual dimension (d) and the number of layers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the average head over d\n",
    "data_d_head: pd.DataFrame = pd.read_csv('scaling_d_head_new.csv')\n",
    "list_d = data_d_head.groupby(['d']).mean()['loss'].index.to_list()\n",
    "for d in list_d:\n",
    "    group = data_d_head[data_d_head['d'] == d].groupby(['nb_head'])\n",
    "    loss_group = group.mean()['loss']\n",
    "    X = loss_group.index.to_list()\n",
    "    Z = loss_group.to_list()\n",
    "    plt.plot(X, Z, label=f'd={d}')\n",
    "plt.xlabel('Number of heads per self-attention')\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.ylabel('KL divergence')\n",
    "plt.title('Scaling law on the residual dimension (d) \\n and the number of heads, with 100 tokens')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the average hidden dimension over the number of parallel heads\n",
    "data_h_para: pd.DataFrame = pd.read_csv('scaling_h_para_new.csv')\n",
    "list_h: List[int] = data_h_para.groupby(['h']).mean()['loss'].index.to_list()\n",
    "for h in list_h:\n",
    "    group = data_h_para[data_h_para['h'] == h].groupby(['para_head'])\n",
    "    loss_group = group.mean()['loss']\n",
    "    X = loss_group.index.to_list()\n",
    "    Z = loss_group.to_list()\n",
    "    plt.plot(X, Z, label=f'h={h}')\n",
    "plt.xlabel('Number of parallel self-attention')\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.ylabel('KL divergence')\n",
    "plt.title('Scaling law on the hidden dimension (h) \\n and the number of parallel self-attention')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the effect of the entropy\n",
    "data_lamb: pd.DataFrame = pd.read_csv('scaling_csv_v3/Scaling_entropy.csv')\n",
    "group = data_lamb.groupby(['lamb'])\n",
    "loss = group.mean()['loss']\n",
    "unif_loss = group.mean()['unif_loss'].to_list()\n",
    "best_loss = group.mean()['best_loss'].to_list()\n",
    "ent = loss.index.to_list()\n",
    "loss = loss.to_list()\n",
    "plt.plot(ent, loss, label='Transformer')\n",
    "plt.plot(ent, unif_loss, label='Uniform', color='red')\n",
    "plt.plot(ent, best_loss, label='Low rank', color='green')\n",
    "plt.xlabel('Entropy of the distribution')\n",
    "plt.ylabel('KL divergence')\n",
    "plt.legend()\n",
    "plt.title('Scaling law on the entropy of the distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the effect of the entropy\n",
    "data_power: pd.DataFrame = pd.read_csv('scaling_csv_v3/scaling_power_law.csv')\n",
    "fig, ax = plt.subplots(1, 3, sharex=True, sharey=True)\n",
    "for i, alpha in enumerate([0.5, 0.9, 0.99]):\n",
    "    group = data_power[data_power['alpha_1'] == alpha].groupby(['alpha_2'])\n",
    "    loss = group.mean()['loss']\n",
    "    unif_loss = group.mean()['unif_loss'].to_list()\n",
    "    best_loss = group.mean()['best_loss'].to_list()\n",
    "    ent = loss.index.to_list()\n",
    "    loss = loss.to_list()\n",
    "    ax[i].plot(ent, loss, label=f'Transformer')\n",
    "    ax[i].plot(ent, unif_loss, label='Uniform', color='red')\n",
    "    ax[i].plot(ent, best_loss, label='Low rank', color='green')\n",
    "    ax[i].set_title(f'alpha={alpha}')\n",
    "    ax[i].set_xlabel('Third token power law parameter')\n",
    "ax[i].legend()\n",
    "ax[0].set_ylabel('KL divergence')\n",
    "fig.suptitle('Scaling laws on the parameters for power laws \\n of parameter alpha for the first and second token')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the effect of the entropy\n",
    "data_bigram: pd.DataFrame = pd.read_csv('scaling_csv_v3/scaling_bigram.csv')\n",
    "colors = list(Color(\"orange\").range_to(Color(\"green\"),5))\n",
    "for i, (d, c) in enumerate(zip([1, 3, 5, 8, 10], colors)):\n",
    "    group = data_bigram[data_bigram['d'] == d].groupby(['alpha_2'])\n",
    "    loss = group.mean()['loss']\n",
    "    unif_loss = group.mean()['unif_loss'].to_list()\n",
    "    best_loss = group.mean()['best_loss'].to_list()\n",
    "    ent = loss.index.to_list()\n",
    "    loss = loss.to_list()\n",
    "    plt.plot(ent, best_loss, label=f'Low rank d={d}', color=c.hex)\n",
    "data_power: pd.DataFrame = pd.read_csv('scaling_csv_v3/scaling_power_law.csv')\n",
    "group = data_power[data_power['alpha_1'] == 0.9].groupby(['alpha_2'])\n",
    "loss = group.mean()['loss'].to_list()\n",
    "plt.plot(ent, loss, label=f'Transformer', color='blue')\n",
    "plt.plot(ent, unif_loss, label='Uniform', color='red')\n",
    "plt.xlabel('Third token power law parameter')\n",
    "plt.ylabel('KL divergence')\n",
    "plt.legend()\n",
    "plt.title('Scaling laws of the third token\\'s power law\\'s parameter \\n and optimal bigram scalings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the average h over layers\n",
    "data_h_layer: pd.DataFrame = pd.read_csv('scaling_h_layer_new.csv')\n",
    "for nb_layers in [1, 2, 4, 5]:\n",
    "    group = data_h_layer[data_h_layer['nb_layers'] == nb_layers].groupby(['h'])\n",
    "    loss_group = group.mean()['loss']\n",
    "    X = [h for h in loss_group.index.to_list()]\n",
    "    Z = loss_group.to_list()\n",
    "    plt.plot(X, Z, label=f'l={nb_layers}')\n",
    "plt.xlabel('Width h of the MLPs')\n",
    "plt.ylabel('KL divergence')\n",
    "plt.title('Scaling law on the number of layer (l) and the width of the MLPs (h)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the average parallel heads\n",
    "data_para: pd.DataFrame = pd.read_csv('scaling_csv_v3/scaling_h_layer_para_2.csv')\n",
    "for h in [0, 100, 1000]:\n",
    "    for layer in [1, 2]:\n",
    "        group = data_para[data_para['h'] == h][data_para['nb_layers'] == layer].groupby(['para_head'])\n",
    "        loss_group = group.mean()['loss']\n",
    "        ent = [h for h in loss_group.index.to_list()]\n",
    "        loss = loss_group.to_list()\n",
    "        plt.plot(ent, loss, label=f'h={h}, nb_layer={layer}')\n",
    "unif_loss = data_para.groupby(['para_head']).mean()['unif_loss'].to_list()\n",
    "best_loss = [data_para['best_loss'][0] for _ in unif_loss]\n",
    "#plt.plot(ent, unif_loss, label='Uniform', color='red')\n",
    "#plt.plot(ent, best_loss, label='Low rank', color='green')\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xlabel('Number of parallel heads')\n",
    "plt.ylabel('KL divergence (min at 1.0)')\n",
    "plt.title('Scaling law on the number of parallel heads, with and without MLPs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the average parallel heads\n",
    "data_para: pd.DataFrame = pd.read_csv('scaling_csv_v3/scaling_h_layer_para_4.csv')\n",
    "for h in [0, 1000]:\n",
    "    group = data_para[data_para['h'] == h].groupby(['nb_layers'])\n",
    "    loss_group = group.mean()['loss']\n",
    "    ent = loss_group.index.to_list()\n",
    "    loss = loss_group.to_list()\n",
    "    plt.scatter(ent, loss, label=f'h={h}, para=50')\n",
    "unif_loss = data_para.groupby(['para_head']).mean()['unif_loss'].to_list()\n",
    "best_loss = [data_para['best_loss'][0] for _ in unif_loss]\n",
    "#plt.plot(ent, unif_loss, label='Uniform', color='red')\n",
    "#plt.plot(ent, best_loss, label='Low rank', color='green')\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xlabel('Number of parallel heads')\n",
    "plt.ylabel('KL divergence')\n",
    "plt.title('Scaling law on the number of parallel heads, with and without MLPs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the average d over N\n",
    "data_d: pd.DataFrame = pd.read_csv('scaling_d_new.csv')\n",
    "for N in [100]:\n",
    "    group = data_d[data_d['N'] == N].groupby(['d'])\n",
    "    loss_group = group.mean()['loss']\n",
    "    X = [d for d in loss_group.index.to_list()]\n",
    "    Z = loss_group.to_list()\n",
    "    #minZ = min(Z)\n",
    "    #Z = [z-minZ for z in Z]\n",
    "    plt.plot(X, Z, label=f'N={N}')\n",
    "    plt.xlabel('d')\n",
    "    plt.ylabel('KL divergence') #(rescaled)')\n",
    "plt.title('Scaling law on the residual dimension (d) and the number of tokens (N)')\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the average parallel heads\n",
    "data_free: pd.DataFrame = pd.read_csv('scaling_csv_v3/scaling_mlp_free.csv')\n",
    "for layer in [1, 2, 3, 5, 8]:\n",
    "    group = data_free[data_free['nb_layers'] == layer].groupby(['para_head'])\n",
    "    loss_group = group.mean()['loss']\n",
    "    ent = loss_group.index.to_list()\n",
    "    loss = loss_group.to_list()\n",
    "    plt.plot(ent, loss, label=f'nb_layer={layer}')\n",
    "#unif_loss = data_free.groupby(['para_head']).mean()['unif_loss'].to_list()\n",
    "#best_loss = [data_free['best_loss'][0] for _ in unif_loss]\n",
    "#plt.plot(ent, unif_loss, label='Uniform', color='red')\n",
    "#plt.plot(ent, best_loss, label='Low rank', color='green')\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xlabel('Number of parallel heads')\n",
    "plt.ylabel('KL divergence (min at 1.0)')\n",
    "plt.title('Scaling law on the number of parallel heads for MLP-free Transformers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the average parallel heads\n",
    "data_free: pd.DataFrame = pd.read_csv('scaling_csv_v3/scaling_mlp_free_3.csv')\n",
    "for layer in [1]:\n",
    "    group = data_free[data_free['nb_layers'] == layer].groupby(['para_head'])\n",
    "    loss_group = group.mean()['loss']\n",
    "    ent = loss_group.index.to_list()\n",
    "    loss = loss_group.to_list()\n",
    "    plt.plot(ent, loss, label=f'nb_layer={layer}')\n",
    "unif_loss = data_free.groupby(['para_head']).mean()['unif_loss'].to_list()\n",
    "best_loss = [data_free['best_loss'][0] for _ in unif_loss]\n",
    "plt.plot(ent, unif_loss, label='Uniform', color='red')\n",
    "plt.plot(ent, best_loss, label='Low rank', color='green')\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xlabel('Number of parallel heads')\n",
    "plt.ylabel('KL divergence (min at 1.0)')\n",
    "plt.title('Scaling law on the number of parallel heads for MLP-free Transformers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
