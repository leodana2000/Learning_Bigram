{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from train import train\n",
    "from models import Transformer, Low_rank\n",
    "from utils import generate_data, entropy, power_unif_law, generate_each, last_position_law, gen_d_law, almost_rank_d\n",
    "from interp import back_track, attention_map, by_attention, every_attention, new_computation_basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training transformer\n",
    "\n",
    "#model params\n",
    "N = 100\n",
    "d = 10\n",
    "nb_layers = 1\n",
    "width = 0\n",
    "depth = 0\n",
    "para = 5\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "#distribution params\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[40, 40, 1]\n",
    "t.manual_seed(2222)\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "#learning params\n",
    "batch_size=2**10\n",
    "num_batch=5000\n",
    "lr=1e-3\n",
    "\n",
    "low_batch_size=2**10\n",
    "low_num_batch=4000\n",
    "low_lr=1e-3\n",
    "\n",
    "device='cpu'\n",
    "cosim = t.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "model = Transformer(d, N, nb_layers, width, depth, para, nb_head, context_window, pi)\n",
    "dict1 = train(model, Data, lr=lr, next_token=False)\n",
    "\n",
    "plt.plot(dict1['Loss'], label=f'Full divergence')\n",
    "print(sum(dict1['Loss'][-100:-1])/100)\n",
    "print(sum(dict1['Acc'][-100:-1])/100)\n",
    "\n",
    "model = Transformer(d, N, nb_layers, width, depth, para, nb_head, context_window, pi)\n",
    "dict2 = train(model, Data, lr=lr, next_token=True)\n",
    "\n",
    "plt.plot(dict2['Loss'], label=f'Next token')\n",
    "print(sum(dict2['Loss'][-100:-1])/100)\n",
    "print(sum(dict2['Acc'][-100:-1])/100)\n",
    "\n",
    "\n",
    "#upper bound\n",
    "ent=entropy(pi)\n",
    "plt.plot([np.log(N)-ent for _ in dict2['Loss']], label='Random baseline', color='red')\n",
    "\n",
    "#lower bound\n",
    "model_low = Low_rank(d, N, context_window, pi)\n",
    "Data = generate_data(low_batch_size, low_num_batch, pi, context_window)\n",
    "dict_low = train(model_low, Data, lr=low_lr)\n",
    "best_loss = sum(dict_low['Loss'][-100:-1])/100\n",
    "plt.plot([best_loss for _ in dict2['Loss']], label='Optimal baseline', color='green')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Divergence\")\n",
    "plt.title(\"Transformer's learning dynamics\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(dict2['Acc'], label=f'Next token')\n",
    "plt.plot(dict1['Acc'], label=f'Full divergence')\n",
    "plt.plot([1/N for _ in dict2['Acc']], color='black', label='Random baseline')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(top=1+0.1, bottom=0-0.1)\n",
    "plt.title(\"Transformer's learning dynamics\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Freezing different components in the training process.\"\"\"\n",
    "\n",
    "#Model parameters\n",
    "N = 10\n",
    "d = 5\n",
    "nb_layers = 1\n",
    "width = 0\n",
    "depth = 0\n",
    "para = 20\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "#Distribution parameters\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[N, N, 1]\n",
    "t.manual_seed(666)\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "#Learning parameters\n",
    "batch_size=2**10\n",
    "num_batch=5000\n",
    "lr=1e-3\n",
    "\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "Freezer = [\n",
    "    (True, True, False, True), \n",
    "    (False, False, True, False),\n",
    "]\n",
    "\n",
    "for freeze_E, freeze_QKV, freeze_O, freeze_U in Freezer:\n",
    "    freezer = {\n",
    "        'freeze_E': freeze_E,\n",
    "        'freeze_QKV': freeze_QKV,\n",
    "        'freeze_O': freeze_O,\n",
    "        'freeze_U': freeze_U,\n",
    "    }\n",
    "    model = Transformer(d, N, nb_layers, width, depth, para, nb_head, context_window, pi)\n",
    "    model.freeze(freezer)\n",
    "    dict = train(model, Data, lr=lr)\n",
    "    print(freezer)\n",
    "    print(sum(dict['Loss'][-100:-1])/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Measuring different scaling laws.\"\"\"\n",
    "\n",
    "loss=[]\n",
    "alpha_1_list=[]\n",
    "alpha_2_list=[]\n",
    "N_list=[]\n",
    "d_list=[]\n",
    "width_list=[]\n",
    "nb_layers_list=[]\n",
    "nb_head_list=[]\n",
    "para_list=[]\n",
    "unif_loss=[]\n",
    "best_loss=[]\n",
    "\n",
    "context_window = 3\n",
    "batch_size = 2**9\n",
    "num_batch = 3000\n",
    "seed=2222\n",
    "\n",
    "count = 0\n",
    "max_count = 99//8 #Counting the number of iteration to keep track of expected time.\n",
    "\n",
    "for N in [100]:\n",
    "    for alpha_1, alpha_2 in product([0.9],[0.6]):\n",
    "        t.manual_seed(seed)\n",
    "        alphas = [alpha_1, alpha_1, alpha_2]\n",
    "        nb_tokens = [N, N, N]\n",
    "        pi = power_unif_law(alphas, nb_tokens, N)\n",
    "        Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "        ent = entropy(pi).item()\n",
    "        for d in [10]:\n",
    "            model_low = Low_rank(N, d, n_gram, context_window, pi)\n",
    "            dict_low = train(model_low, Data, lr=1e-3)\n",
    "            best = sum(dict_low['Loss'][-100:-1])/100\n",
    "            for width in [0]:\n",
    "                for depth in [1]:\n",
    "                    for nb_head in [1]:\n",
    "                        for nb_layers in [1]:\n",
    "                            for parallel_heads in [5]:\n",
    "                                N_list.append(N)\n",
    "                                d_list.append(d)\n",
    "                                width_list.append(width)\n",
    "                                nb_head_list.append(nb_head)\n",
    "                                nb_layers_list.append(nb_layers)\n",
    "                                para_list.append(parallel_heads)\n",
    "                                alpha_1_list.append(alpha_1)\n",
    "                                alpha_2_list.append(alpha_2)\n",
    "                                unif_loss.append(-ent+np.log(N))\n",
    "                                best_loss.append(best)\n",
    "\n",
    "                                model = Transformer(d, N, nb_layers, width, depth, para, nb_head, context_window, pi)\n",
    "                                dict = train(model, Data, lr=1e-3)\n",
    "                                loss.append(sum(dict['Loss'][-100:-1])/100)\n",
    "\n",
    "                                count+=1\n",
    "                                print(count/max_count)\n",
    "\n",
    "dict={\n",
    "    'alpha_1': alpha_1_list,\n",
    "    'alpha_2': alpha_2_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'width': width_list,\n",
    "    'nb_layers': nb_layers_list,\n",
    "    'nb_head': nb_head_list,\n",
    "    'para_head': para_list,\n",
    "    'loss': loss,\n",
    "    'unif_loss': unif_loss,\n",
    "    'best_loss': best_loss,\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(dict)\n",
    "data.to_csv('scaling_csv_v3/.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Measuring the effect of different architectural modifications.\"\"\"\n",
    "\n",
    "#Model parameters\n",
    "N = 100\n",
    "d = 10\n",
    "h = 0\n",
    "depth = 1\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "para = 10\n",
    "n_gram = 3\n",
    "max_seq_len = n_gram\n",
    "assert max_seq_len >= n_gram\n",
    "assert n_gram == 3\n",
    "\n",
    "#Distribution parameters\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[40, 40, 1]\n",
    "t.manual_seed(666)\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "#Learning parameters\n",
    "batch_size=2**10\n",
    "num_batch=5000\n",
    "lr=1e-3\n",
    "seed=333\n",
    "\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "for skip_res_connection in [False, True]:\n",
    "    for skip_pos_QK in [False]:\n",
    "        for skip_emb_QK in [False]:\n",
    "            for skip_pos_OV in [False]:\n",
    "                for skip_emb_OV in [False]:\n",
    "                    skips = {\n",
    "                                'skip_res_connection': skip_res_connection,\n",
    "                                'skip_pos_QK': skip_pos_QK,\n",
    "                                'skip_emb_QK': skip_emb_QK,\n",
    "                                'skip_pos_OV': skip_pos_OV,\n",
    "                                'skip_emb_OV': skip_emb_OV,\n",
    "                            }\n",
    "                    t.manual_seed(seed)\n",
    "                    model = Transformer(d, N, nb_layers, width, depth, para, nb_head, context_window, pi)\n",
    "                    if not((skip_pos_OV and skip_emb_OV) or (skip_pos_QK and skip_emb_QK)):\n",
    "                        dict = train(model, Data, lr=lr)\n",
    "                        print(skips)\n",
    "                        print(sum(dict['Loss'][-100:-1])/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Looking at the shape of the learned attention pattern.\"\"\"\n",
    "\n",
    "#Model parameters\n",
    "N = 100\n",
    "d = 10\n",
    "nb_layers = 1\n",
    "width = 0\n",
    "depth = 1\n",
    "para = 3\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "#Learning parameters\n",
    "batch_size=2**10\n",
    "num_batch=5000\n",
    "lr=1e-3\n",
    "\n",
    "#Distribution parameters\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[10, 10, 1]\n",
    "t.manual_seed(666)\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "skips = {\n",
    "    'skip_res_connection': True, #We skip the residual connection.\n",
    "    'skip_pos_QK': False,\n",
    "    'skip_emb_QK': False,\n",
    "    'skip_pos_OV': False,\n",
    "    'skip_emb_OV': False,\n",
    "}\n",
    "\n",
    "model = Transformer(d, N, nb_layers, width, depth, para, nb_head, context_window, pi, skips=skips)\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "dict = train(model, Data, lr=lr)\n",
    "print(sum(dict['Loss'][-100:-1])/100)\n",
    "\n",
    "for i in range(para):\n",
    "    map = attention_map(model, 0, i).detach()\n",
    "    sns.heatmap(map, vmin=0, vmax=1)\n",
    "    plt.xlabel('Position 1')\n",
    "    plt.ylabel('Position 2')\n",
    "    plt.title(f\"Probability of head {i} given to the first token for an input pair.\")\n",
    "    plt.show()\n",
    "\n",
    "    S = t.linalg.svdvals(map)\n",
    "    print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Measuring each head's contribution to the next token prediction.\"\"\" #TODO: clean\n",
    "\n",
    "#Model parameters\n",
    "N = 10\n",
    "d = 5\n",
    "nb_layers = 1\n",
    "width = 0\n",
    "depth = 0\n",
    "para = 2\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "#Learning parameters\n",
    "batch_size=2**9\n",
    "num_batch=4000\n",
    "lr=1e-3\n",
    "\n",
    "#Distribution parameters\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[10, 5, 1]\n",
    "seed = 2222\n",
    "t.manual_seed(seed)\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "skips = {\n",
    "    'skip_res_connection': True, #We skip the residual connection.\n",
    "    'skip_pos_QK': False,\n",
    "    'skip_emb_QK': False,\n",
    "    'skip_pos_OV': False,\n",
    "    'skip_emb_OV': False,\n",
    "}\n",
    "\n",
    "model = Transformer(d, N, nb_layers, width, depth, para, nb_head, context_window, pi, skips=skips)\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "dict = train(model, Data, lr=lr, seed=seed)\n",
    "print(sum(dict['Loss'][-10:-1])/10)\n",
    "\n",
    "examples = generate_each(pi)\n",
    "contribution = back_track(model, examples)\n",
    "new_contribution = new_computation_basis(model, examples)\n",
    "\n",
    "ex = 10\n",
    "print(new_contribution[f'para_{0}_layer_{0}'][ex, 2], t.norm(new_contribution[f'para_{0}_layer_{0}'][ex, 2]))\n",
    "print(new_contribution[f'para_{1}_layer_{0}'][ex, 2], t.norm(new_contribution[f'para_{1}_layer_{0}'][ex, 2]))\n",
    "print(new_contribution[f'para_{1}_layer_{0}'][ex, 2]+new_contribution[f'para_{0}_layer_{0}'][ex, 2])\n",
    "\n",
    "W_U = model.unemb.weight.detach()\n",
    "map = t.einsum('Nd, nd -> Nn', W_U, W_U)\n",
    "sns.heatmap(map, center=0, cmap='bwr')\n",
    "plt.title(r'$W_U^TW_U$')\n",
    "plt.show()\n",
    "\n",
    "map = map*(new_contribution[f'para_{1}_layer_{0}'][ex, 2]+new_contribution[f'para_{0}_layer_{0}'][ex, 2]).unsqueeze(0)\n",
    "sns.heatmap(map, center=0, cmap='bwr')\n",
    "plt.title(r'$W_U^TW_UX$')\n",
    "plt.show()\n",
    "\n",
    "every_attention(contribution, examples[ex].unsqueeze(0), 0)\n",
    "\n",
    "_, computations = model.forward(examples, out_computation=True)\n",
    "for i in range(N):\n",
    "    #print(computations['logits'][examples[:, 2] == 0].mean(0)[2] - (computations['logits'][examples[:, 2] == 0].mean(0)[2]).mean())\n",
    "    A = W_U@computations[f'para_{0}_layer_{0}'][examples[:, 2] == i].mean(0)[2] - (W_U@computations[f'para_{0}_layer_{0}'][examples[:, 2] == i].mean(0)[2]).mean()\n",
    "    B = W_U@computations[f'para_{1}_layer_{0}'][examples[:, 2] == i].mean(0)[2] - (W_U@computations[f'para_{1}_layer_{0}'][examples[:, 2] == i].mean(0)[2]).mean()\n",
    "    #print(A)\n",
    "    #print(B)\n",
    "    print(t.logical_and(t.logical_and(A*B > 0, A > 0), B > 0))\n",
    "\n",
    "for i in range(para):\n",
    "    by_attention(contribution, examples, 0, i, sort=False, method='solo')\n",
    "\n",
    "by_attention(contribution, examples, 0, i, sort=False, method=\"group\")\n",
    "\n",
    "map = attention_map(model, 0, 0).detach()\n",
    "sns.heatmap(map, vmin=0, vmax=1)\n",
    "plt.xlabel('Position 1')\n",
    "plt.ylabel('Position 2')\n",
    "plt.title(f\"Probability of head {0} given to the first token for an input pair.\")\n",
    "plt.show()\n",
    "\n",
    "S = t.linalg.svdvals(map)\n",
    "print(S)\n",
    "\n",
    "map = attention_map(model, 0, 1).detach()\n",
    "sns.heatmap(map, vmin=0, vmax=1)\n",
    "plt.xlabel('Position 1')\n",
    "plt.ylabel('Position 2')\n",
    "plt.title(f\"Probability of head {1} given to the first token for an input pair.\")\n",
    "plt.show()\n",
    "\n",
    "S = t.linalg.svdvals(map)\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Show the Superposition phenomenon one low entropy distribution.\"\"\" #TODO: clean\n",
    "\n",
    "#Model parameters\n",
    "N = 10\n",
    "d = 5\n",
    "context_window = 3\n",
    "\n",
    "#Learning parameters\n",
    "batch_size=2**9\n",
    "num_batch=5000\n",
    "lr=1e-3\n",
    "\n",
    "t.manual_seed(2108) #2108 and 1\n",
    "nb_tokens = [N, N]\n",
    "pi = last_position_law([N, N], N, 0, 0.95)\n",
    "\n",
    "model = Low_rank(d, N, context_window, pi)\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "dict = train(model, Data, lr=lr)\n",
    "print(sum(dict['Loss'][-100:-1])/100)\n",
    "\n",
    "\n",
    "W_U = model.unemb.weight.detach()\n",
    "W_U = W_U#/t.norm(W_U, dim=-1, keepdim=True)\n",
    "map = t.einsum('Nd, nd -> Nn', W_U, W_U)\n",
    "sns.heatmap(map, center=0, cmap='bwr')\n",
    "plt.title(r'$W_U^TW_U$ for quadratic importance')\n",
    "plt.show()\n",
    "\n",
    "W_E = model.word_emb.weight.detach()\n",
    "w_e = []\n",
    "examples = generate_each(pi, eps=0.1/(N**2))\n",
    "indices = []\n",
    "for ex in examples:\n",
    "    indices.append(ex[0]+ex[1]*N)\n",
    "indices = t.Tensor(indices).to(t.int)\n",
    "W_E = W_E[indices]#/t.norm(W_E[indices], dim=-1, keepdim=True)\n",
    "map = W_E@W_E.mH\n",
    "sns.heatmap(map, center=0, cmap='bwr')\n",
    "plt.title(r'$W_E^TW_E$')\n",
    "plt.show()\n",
    "\n",
    "#Prints the cosimilarity between the w_e(z) and the expected right w_u(g(z))\n",
    "print(((W_U/t.norm(W_U, dim=-1, keepdim=True))@(W_E/t.norm(W_E, dim=-1, keepdim=True)).mH)[examples[t.arange(N**2), 2], t.arange(N**2)])\n",
    "\n",
    "model = Transformer(d, N, nb_layers=1, width=0, depth=0, parallel_heads=5, nb_head=1, context_window=context_window, pi=pi)\n",
    "#model.unemb.weight = t.nn.Parameter(W_U, requires_grad=False)\n",
    "dict = train(model, Data, lr=lr)\n",
    "print(sum(dict['Loss'][-100:-1])/100)\n",
    "\n",
    "W_U = model.unemb.weight.detach()\n",
    "W_U = W_U#/t.norm(W_U, dim=-1, keepdim=True)\n",
    "map = t.einsum('Nd, nd -> Nn', W_U, W_U)\n",
    "sns.heatmap(map, center=0, cmap='bwr')\n",
    "plt.title(r'$W_U^TW_U$ for quadratic importance')\n",
    "plt.show()\n",
    "\n",
    "with t.no_grad():\n",
    "    _, computations = model.forward(examples, out_computation=True)\n",
    "W_E = computations[f'res_after_mlp_layer_{0}'].detach()[:, 2, :]\n",
    "W_E = W_E[indices]#/t.norm(W_E[indices], dim=-1, keepdim=True)\n",
    "map = W_E@W_E.mH\n",
    "sns.heatmap(map, center=0, cmap='bwr')\n",
    "plt.title(r'$A^TA$')\n",
    "plt.show()\n",
    "\n",
    "#Prints the cosimilarity between the w_e(z) and the expected right w_u(g(z))\n",
    "print(((W_U/t.norm(W_U, dim=-1, keepdim=True))@(W_E/t.norm(W_E, dim=-1, keepdim=True)).mH)[examples[t.arange(N**2), 2], t.arange(N**2)])\n",
    "\n",
    "\n",
    "#Prints the cosimilarity between all pairs W_E + POS\n",
    "W_E = model.word_emb.weight.detach()\n",
    "POS = model.pos_emb.weight.detach()[:2]\n",
    "W_E = (W_E.unsqueeze(0) + POS.unsqueeze(1)).flatten(0, 1)\n",
    "map = W_E@W_E.mH\n",
    "sns.heatmap(map, center=0, cmap='bwr')\n",
    "plt.title(r'$(W_E+POS)^T(W_E+POS)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:06<00:00, 724.16it/s]\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 704.32it/s]\n",
      "100%|██████████| 5000/5000 [00:06<00:00, 714.74it/s]\n",
      " 33%|███▎      | 1659/5000 [00:02<00:04, 693.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m Low_rank(d, N, context_window, pi)\n\u001b[1;32m     45\u001b[0m Data \u001b[38;5;241m=\u001b[39m generate_data(batch_size\u001b[38;5;241m=\u001b[39mbatch_size, num_batch\u001b[38;5;241m=\u001b[39mnum_batch, pi\u001b[38;5;241m=\u001b[39mpi, context_window\u001b[38;5;241m=\u001b[39mcontext_window)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m W_U \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39munemb\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     49\u001b[0m mean \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mnorm(W\u001b[38;5;129m@t\u001b[39m\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(W\u001b[38;5;241m.\u001b[39mmH\u001b[38;5;129m@W\u001b[39m)\u001b[38;5;129m@W\u001b[39m\u001b[38;5;241m.\u001b[39mmH\u001b[38;5;241m-\u001b[39mW_U\u001b[38;5;129m@t\u001b[39m\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(W_U\u001b[38;5;241m.\u001b[39mmH\u001b[38;5;129m@W_U\u001b[39m)\u001b[38;5;129m@W_U\u001b[39m\u001b[38;5;241m.\u001b[39mmH)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/Stage 4/Learning_Bigram/train.py:42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, lr, seed)\u001b[0m\n\u001b[1;32m     40\u001b[0m Loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m Acc \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 42\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43macc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompute_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#incorrect\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/leo_env/lib/python3.11/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/leo_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/leo_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/leo_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/leo_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/leo_env/lib/python3.11/site-packages/torch/utils/data/dataset.py:208\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "File \u001b[0;32m~/anaconda3/envs/leo_env/lib/python3.11/site-packages/torch/utils/data/dataset.py:208\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Show that near \"\"\" #TODO: clean\n",
    "\n",
    "N = 10\n",
    "d = 5\n",
    "context_window = 3\n",
    "\n",
    "#learning params\n",
    "batch_size=2**9\n",
    "num_batch=5000\n",
    "lr=1e-3\n",
    "\n",
    "#distribution params\n",
    "axis_aligned = []\n",
    "non_axis_aligned = []\n",
    "random = []\n",
    "nb_tokens=[N, N]\n",
    "num_rep = 3\n",
    "t.manual_seed(666)\n",
    "for dim in range(d, N+1):\n",
    "    mean = 0.\n",
    "    for _ in range(num_rep):\n",
    "        pi, W = gen_d_law(nb_tokens, N, dim, axis_aligned=True)\n",
    "        U, S, V = t.linalg.svd(W)\n",
    "        W = (U[:, :d]*S[:d])@V[:d, :d]\n",
    "\n",
    "        model = Low_rank(d, N, context_window, pi)\n",
    "        Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "        dict = train(model, Data, lr=lr, seed=(_+1)*(dim+1))\n",
    "\n",
    "        W_U = model.unemb.weight.detach()\n",
    "        mean += t.norm(W@t.linalg.inv(W.mH@W)@W.mH-W_U@t.linalg.inv(W_U.mH@W_U)@W_U.mH).item()\n",
    "    axis_aligned.append(mean/num_rep)\n",
    "\n",
    "    mean = 0.\n",
    "    for _ in range(num_rep):\n",
    "        pi, W = gen_d_law(nb_tokens, N, dim, axis_aligned=False)\n",
    "        U, S, V = t.linalg.svd(W)\n",
    "        W = (U[:, :d]*S[:d])@V[:d, :d]\n",
    "\n",
    "        model = Low_rank(d, N, context_window, pi)\n",
    "        Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "        dict = train(model, Data, lr=lr, seed=(_+1)*(dim+1))\n",
    "\n",
    "        W_U = model.unemb.weight.detach()\n",
    "        mean += t.norm(W@t.linalg.inv(W.mH@W)@W.mH-W_U@t.linalg.inv(W_U.mH@W_U)@W_U.mH).item()\n",
    "    non_axis_aligned.append(mean/num_rep)\n",
    "\n",
    "    mean = 0.\n",
    "    for _ in range(30):\n",
    "        Q = t.randn_like(W)\n",
    "        mean += t.norm(Q@t.linalg.inv(Q.mH@Q)@Q.mH-W@t.linalg.inv(W.mH@W)@W.mH).item()\n",
    "    random.append(mean/30)\n",
    "\n",
    "X = [dim for dim in range(d, N+1)]\n",
    "plt.plot(X, axis_aligned, label='axis aligned')\n",
    "plt.plot(X, non_axis_aligned, label='axis unaligned')\n",
    "plt.plot(X, random, label='random baseline')\n",
    "plt.legend()\n",
    "plt.xlabel('Embedding dimension')\n",
    "plt.ylabel('L2 matrix distance')\n",
    "plt.title('Distance between W_U and low rank log-prob')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:11<00:00, 421.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026685121059417723\n",
      "tensor([ 1.0415,  1.9363,  3.1598,  5.0069,  1.9707,  1.5509, 12.1780,  2.3745,\n",
      "         5.9765,  5.2128,  3.2085,  1.0149,  1.8449, 11.2013, 13.2905,  3.2395,\n",
      "         7.7813,  3.9956,  1.3935, 13.2777,  2.3417,  1.5386,  1.0061,  3.3038,\n",
      "         7.0918,  4.2550,  5.5623,  4.0351,  1.6059,  3.8768,  3.6817, 14.5698,\n",
      "         4.6992,  1.0136,  3.2237,  1.2154,  3.1014,  2.1661,  1.7571, 17.6890,\n",
      "         2.9061, 17.2756,  3.9689,  3.5767,  1.0110,  1.8904,  5.1125,  4.2290,\n",
      "         2.6369,  3.5960,  1.4123,  2.9827,  7.4260,  1.2031,  1.8818,  1.0039,\n",
      "         1.2683,  6.4286,  6.0821,  2.9564, 18.0613,  6.1706,  3.1082,  3.6319,\n",
      "         8.0174,  1.2654,  1.0123,  2.1825,  2.7111,  4.1964,  2.7695,  4.0028,\n",
      "         6.4231,  2.2761,  3.9487,  8.7244,  2.2082,  1.0183,  4.2089,  3.5555,\n",
      "        17.7348,  1.4154,  1.5374,  2.2965,  4.1503,  2.8886,  2.9644,  3.0730,\n",
      "         1.1668,  1.5287,  6.6125, 10.9166,  5.9564, 10.5734,  2.3835,  2.6210,\n",
      "         3.6969,  4.0063,  2.5921,  1.0391])\n",
      "tensor(1.1910)\n",
      "tensor(0.0149)\n",
      "tensor(0.0192)\n",
      "tensor([0.2585, 0.0233, 0.0743, 0.0143, 0.0189, 0.0263, 0.0569, 0.0378, 0.0274,\n",
      "        0.4623]) tensor([0.2661, 0.0105, 0.0568, 0.0296, 0.0054, 0.0113, 0.0716, 0.0360, 0.0401,\n",
      "        0.4725]) tensor([0.2760, 0.0101, 0.0457, 0.0272, 0.0063, 0.0102, 0.0913, 0.0317, 0.0409,\n",
      "        0.4607])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#d direction but nice #TODO: clean\n",
    "N = 10\n",
    "d = 5\n",
    "context_window = 3\n",
    "\n",
    "#learning params\n",
    "batch_size=2**10\n",
    "num_batch=5000\n",
    "lr=5e-4\n",
    "\n",
    "#distribution params\n",
    "axis_aligned = []\n",
    "non_axis_aligned = []\n",
    "random = []\n",
    "nb_tokens=[N, N]\n",
    "t.manual_seed(66)\n",
    "\n",
    "pi = almost_rank_d(nb_tokens, N, d+1, axis_aligned=False)\n",
    "\n",
    "model = Low_rank(d, N, context_window, pi)\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "dict = train(model, Data, lr=lr)\n",
    "print(sum(dict['Loss'][-100:-1])/100)\n",
    "\n",
    "W_E = model.word_emb.weight.detach()\n",
    "W_U = model.unemb.weight.detach()\n",
    "\n",
    "f1 = W_E@W_U.mH #real\n",
    "L = t.log(pi[2].flatten(0, 1))-t.log(pi[2].flatten(0,1)).mean(-1, keepdim=True)\n",
    "PI = pi[2].flatten(0, 1)\n",
    "Z = f1 - L - ((f1-L)*PI).sum(-1, keepdim=True)\n",
    "print(t.log((t.exp(Z)*PI).sum(-1)).mean())\n",
    "\n",
    "EL = (L*PI).sum(-1, keepdim=True)\n",
    "COV = W_U.mH@((L*PI-EL*PI).mH)\n",
    "VAR = t.einsum('Nd, ND, MN -> MdD', W_U, W_U, PI) - t.einsum('Nd, nD, MN, Mn -> MdD', W_U, W_U, PI, PI)\n",
    "w_e = t.einsum('MdD, dM -> MD', t.linalg.inv(VAR), COV)\n",
    "cosim = t.nn.CosineSimilarity(dim=-1)\n",
    "#print(cosim(w_e, W_E))\n",
    "\n",
    "f2 = w_e@W_U.mH #th\n",
    "Z = f2 - L - ((f2-L)*PI).sum(-1, keepdim=True)\n",
    "print(t.log((t.exp(Z)*PI).sum(-1)).mean())\n",
    "\n",
    "U, S, V = t.linalg.svd(t.log(pi[2].flatten(0,1))-t.log(pi[2].flatten(0,1)).mean(-1, keepdim=True))\n",
    "f = t.einsum('Md, d, dN -> MN', U[:, :d], S[:d], V[:d]) #least-square\n",
    "Z = f - L - ((f-L)*PI).sum(-1, keepdim=True)\n",
    "print(t.log((t.exp(Z)*PI).sum(-1)).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
