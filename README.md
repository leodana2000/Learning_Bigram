# Memorization in Attention-only Transformers
This reprository contains my work done at MILES for the Spring internship Feb-June 2024, with Yann Chevaleyre and Muni Sreenivas Pydi.

The goal of the project is to study the memorization capabilities of Toy model Transformers, and especially if attention can memorize, and how well it memorizes compared to MLPs. \
See the report XXX, for more details on the project.

You can test each module by running *tests.py*, and test how the code base works in *test_notebook.ipynb*.

The main notebook is *experiments.ipynb* and gathers all experiments, the results' of which are in the folder *Images*.