{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves to reproduce the experiments done in the main paper, and detailed in appendix B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "t.set_num_threads(8)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from train import train\n",
    "from models import Transformer, Low_rank, AoT\n",
    "from utils import generate_data, entropy, power_unif_law\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is used to train a single AoT or Transformer, and plot the loss and accuracy. \n",
    "A sequence encoder is also trained as a comparison for what the Transformer can achieve at most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training a Transformer.\"\"\"\n",
    "t.manual_seed(2222)\n",
    "\n",
    "# Transformer parameters.\n",
    "N = 50\n",
    "d = 10\n",
    "nb_layers = 1\n",
    "width = 2000\n",
    "para = 1\n",
    "d_head = 10\n",
    "nb_head = 1         # This is the number of head per attention module, thus d_head%nb_head = 0.\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters. Each distribution level is a power law alpha over some tokens. \n",
    "# The parameter nb_tokens says how many tokens are non-zero at each step. \n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[N, N, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Learning parameters for the Transformer.\n",
    "batch_size=2**10\n",
    "num_batch=1000\n",
    "epoches=10\n",
    "lr=1e-3\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "# Training the Transformer.\n",
    "model = Transformer(d, N, nb_layers, width, para, d_head, nb_head, context_window, pi)\n",
    "Dict = train(model, Data, epoches, lr=lr, next_token=True)\n",
    "plt.plot(Dict['Loss'])\n",
    "\n",
    "# Upper bound: we compute the divergence with the uniform predictor.\n",
    "ent=entropy(pi)\n",
    "plt.plot([np.log(N)-ent for _ in Dict['Loss']], label='Uniform baseline', color='red')\n",
    "\n",
    "# Learning parameters for the sequence encoder.\n",
    "low_batch_size=2**10\n",
    "low_num_batch=1000\n",
    "low_lr=1e-3\n",
    "epochs=4\n",
    "\n",
    "# Lower bound: we compute the best Sequence encoder, the diverge of which sets the attainable lower bound.\n",
    "model_low = Low_rank(d, N, context_window, pi)\n",
    "Data = generate_data(low_batch_size, low_num_batch, pi, context_window)\n",
    "dict_low = train(model_low, Data, epochs, lr=low_lr)\n",
    "best_loss = sum(dict_low['Loss'][-101:-1])/100\n",
    "plt.plot([best_loss for _ in Dict['Loss']], label='Optimal baseline', color='green')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Divergence\")\n",
    "plt.title(\"Transformer's learning dynamic\")\n",
    "plt.show()\n",
    "\n",
    "# We plot the accuracy of the Transformer, the accuracy of the random predictor, \n",
    "# and the lower bound from are paper as well as the previous sota bound (in accuracy, not in worst-case).\n",
    "plt.plot(Dict['Acc'], label=f'Next token')\n",
    "plt.plot([1/N for _ in Dict['Acc']], color='black', label='Random baseline')\n",
    "if width == 0: # Put the lower bounds only if we are training and AoT.\n",
    "    plt.plot([1/N+(1-1/N)*para*d_head/(N**(n_gram-1)) for _ in Dict['Acc']], label='Our Lower bound')\n",
    "    plt.plot([1/N+(1-1/N)*(para*(d_head-1)+1)/(N**(n_gram-1)) for _ in Dict['Acc']], label='Previous lower bound')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(top=1+0.1, bottom=0-0.1)\n",
    "plt.title(\"Transformer's learning dynamic\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cells below to produce the data obtained in the folder Scaling laws. Experiments 1 to 4 are done exclusively on AoT, while experiment 5 involves general Transformers.\n",
    "\n",
    "**Experiment 1**: we test the scaling in the variable $H$ (or *para* in the notebook), which is the number of heads.\n",
    "Result: the scaling is found to be linear, as expected.\n",
    "\n",
    "**Experiment 2**: we test the scaling of the variable $d_{h}$, which is the dimension of each head.\n",
    "Result: the scaling is linear as expected.\n",
    "\n",
    "**Experiment 3**: we test the scaling of the variable $d$, which is the embedding dimension.\n",
    "Result: the scaling is linear by part, being separated at $d=d_{h}$. The second linear scale is noisy, meaning that this might be an optimization issue.\n",
    "\n",
    "**Experiment 4**: we test the scaling of $d=d_h$, when heads have the same dimension as the residual stream. (The data used in experiment 4 is the same as experiment 1.)\n",
    "Result: We find a scaling which is cubic. The scaling as expected to be at least quadratic, using the results from experiment 2 and 3. The scaling being cubic might suggest that experiment 3 had indeed optimization issues, which were uplifted by taking $d=d_h$.\n",
    "\n",
    "**Experiment 5**: we test the scaling of a Transformer with one attention head, and an MLP with varying width.\n",
    "Result: we find that the accuracy, by parameter, of both the AoT and the MLP-based Transformer are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 1. Scaling laws on H and d=d_head. \"\"\"\n",
    "t.manual_seed(2222)\n",
    "\n",
    "# Model parameters.\n",
    "N = 50\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[100, 100, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**10\n",
    "num_batch=1000\n",
    "lr=1e-3\n",
    "epochs=10\n",
    "repetition = 2\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "# Scaling parameters\n",
    "min_para=1\n",
    "max_para=31\n",
    "para_step=5\n",
    "min_d=3\n",
    "max_d=13\n",
    "d_step=1\n",
    "\n",
    "for i, d in enumerate(range(min_d, max_d+1, d_step)):\n",
    "    d_head=d \n",
    "\n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "\n",
    "    for para in tqdm(range(min_para, max_para+1, para_step)):\n",
    "        accuracy = 0\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        para_list.append(para)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_1_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 2. Scaling laws on d_head, with d!=d_head and H (=para) fixed. \"\"\"\n",
    "t.manual_seed(2222)\n",
    "\n",
    "# Model parameters.\n",
    "N = 50\n",
    "d = 10\n",
    "para = 20\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[100, 100, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**10\n",
    "num_batch=1000\n",
    "lr=1e-3\n",
    "epochs=10\n",
    "repetition = 2\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "# Scaling parameters\n",
    "min_d_head = 5\n",
    "max_d_head = 15\n",
    "d_head_step = 1\n",
    "\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "d_head_list = []\n",
    "for d_head in tqdm(range(min_d_head, max_d_head+1, d_head_step)):\n",
    "    accuracy = 0\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "        \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    d_head_list.append(d_head)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 3. Scaling laws on  d, with d_head and H (=para) fixed. \"\"\"\n",
    "t.manual_seed(3333)\n",
    "\n",
    "# Model parameters.\n",
    "N = 50\n",
    "d_head = 10\n",
    "para = 20\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[100, 100, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**10\n",
    "num_batch=1000\n",
    "lr=1e-3\n",
    "epochs=10\n",
    "repetition = 2\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "# Scaling parameters\n",
    "min_d = 5\n",
    "max_d = 25\n",
    "d_step = 1\n",
    "\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "d_head_list = []\n",
    "for d in tqdm(range(min_d, max_d+1, d_step)):\n",
    "    accuracy = 0\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    d_head_list.append(d_head)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 5. Scaling laws on the width of Transformer using MLPs. \"\"\"\n",
    "t.manual_seed(3333)\n",
    "\n",
    "# Model parameters.\n",
    "N = 50\n",
    "para = 1\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1, 1, 1]\n",
    "nb_tokens=[100, 100, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**10\n",
    "num_batch=1000\n",
    "lr=1e-3\n",
    "epochs=10\n",
    "repetition = 2\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "for d in [5, 10, 13]:\n",
    "    d_head = d\n",
    "    min_width = 2*d*1\n",
    "    max_width = 2*d*26\n",
    "    step = 2*d*5\n",
    "\n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "    width_list = []\n",
    "    for width in tqdm(range(min_width, max_width+1, step)):\n",
    "        accuracy = 0\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            model = Transformer(d, N, nb_layers, width, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "            print(accuracy)\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        para_list.append(para)\n",
    "        width_list.append(width)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "        'width': width_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_5_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
